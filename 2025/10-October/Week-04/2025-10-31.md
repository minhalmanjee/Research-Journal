# Daily Log - 31-October
# Chap 16

Fault tolerance – if one copy fails, others can serve data


Availability – users can access data anytime, anywhere


Performance – users access nearby copies, reducing delay


Reliability → data or service is correct and dependable (no corruption).


Availability → data or service is accessible when needed.

Replication Transparency
Users shouldn’t need to know that multiple copies exist—they should feel like they’re accessing a single, reliable system.

1. Passive Replication (Primary–Backup Model)
One primary server handles all client requests.


Backup servers keep synchronized copies.


When the primary fails, a backup is promoted to primary (after detecting failure via heartbeat messages).


The time between crash and recovery is the failover time.


Needs at least (m + 1) replicas to tolerate m crashes.


2. Active Replication (State Machine Model)
All replicas process every client request.


Each replica maintains the same sequence of operations using total order multicast, so their states remain identical.


More fault-tolerant but requires strict coordination.

Consistency Models
1. Strict Consistency: 
The strongest model 
all processes see updates instantly.
If one process updates x = 5 at time t, then all others will see x = 5 immediately after t.
This matches real-time ordering.
Not practical in real distributed systems because instantaneous updates are impossible (message delays).

2. Linearizability
A bit weaker than strict consistency.
Operations appear to occur in one global order that respects real-time order, but small propagation delays are tolerated. Each read returns the most recent write according to that global order.
3. Sequential Consistency: Even weaker than linearizability. All processes see writes in the same order, Internal ordering (within each process) must still be preserved
Example: If all customers see the seat count decrease from 100 → 99 → 98 (same sequence), it’s fine. But if some see 99 before others see 100, it breaks sequential consistency.

4. Causal Consistency
Focuses on cause-and-effect relationships between operations.
If one write causes another (e.g., one user replies to another’s post), all processes must see them in that same order. Writes that are independent can appear in different orders to different processes.
5. FIFO Consistency (First-In, First-Out)
Even weaker than causal consistency. Only ensures that each process’s own writes are seen by others in the order issued. Different processes’ writes may be seen in different orders.

“Eventual consistency” means:
If no one makes any more changes, then eventually everyone will see the same, correct data.
The Domain Name System (DNS) (which connects website names to IP addresses) uses eventual consistency.
If a website’s address changes, it might take a few hours for that update to spread to every server around the world — but eventually, they’ll all show the new address.

For mobile clients:


Read-After-Read (Monotonic Read) Consistency

If you read something once, then read it again from another location,
you should never see older data than before.

Write-After-Write (Monotonic Write) Consistency

If you make several updates in different places,
those updates should happen in the same order everywhere.

Read-After-Write (Read Your Writes) Consistency

After you update something, you should be able to see your own change the next time you read it.

Write-After-Read Consistency

If you first read some data and then write (update) it,
the write must be based on the latest version you read.






Model
Meaning
Example
Eventual Consistency
Everyone sees the same data eventually
DNS updates
Read-After-Read
You never see older data than before
Reading emails in two cities
Write-After-Write
Your updates happen in the right order
Editing salaries then bonuses
Read-After-Write
You can see your own recent updates
Changing your password
Write-After-Read
Updates happen on the latest version you saw
Bank balance and purchase



# Paper 01:

Preliminaries:
We consider a distributed system consisting of a finite set of processes
These processes interact with shared objects
It’s asynchronous, meaning there’s no fixed timing it can take any duration to deliver
The system runs asynchronously (no fixed timing), but researchers use an imaginary global clock to talk about how recent or stale operations are when studying consistency.
Replica refers to copy of the object.


rb (returns-before)

This says one operation finished before another started in real time.

Example: if operation a finishes before b begins, then a rb→ b.

This gives a partial order (some operations are ordered, some are not if they overlap).

🔹 Formally: rb = { (a, b) | a.rtime < b.stime }
(meaning “a’s return time < b’s start time”)

ss (same-session)

Groups together all operations done by the same process (or client).

Example: all actions performed by user A form one session.

🔹 Formally: ss = { (a, b) | a.proc = b.proc }
→ So if both operations come from the same process, they are in the same session.

so (session order)

Combines the two above: it’s the order of operations within the same session, according to real time.

In other words, it tells us the order in which a single client’s operations occurred.

🔹 Formally: so = rb ∩ ss
(means “returns-before” and “same-session” at the same time)

ob (same-object)

Groups operations that act on the same shared object (e.g., the same data item).

Example: all reads/writes to “object X” are in the same equivalence group.

🔹 Formally: ob = { (a, b) | a.obj = b.obj }

concur (concurrent)

Shows pairs of operations on the same object that happened at the same time (i.e., overlapping in real time).

Example: if two clients update the same record almost simultaneously, their operations are concurrent.

🔹 Formally: concur = ob \ rb
(means “on the same object” but not ordered by real time)

a rel→ b → means a is related to b by relation rel (e.g., a rb→ b).

rel⁻¹ → means the inverse of that relation.

If two operations are equivalent under relation rel, we write a ≈rel b.

H / ≈rel → means dividing (or partitioning) the set of operations into groups (equivalence classes) based on that relation.




1. Visibility (vis)

This is a relation that tells us when one operation’s effects are seen by another.

If a →vis b, it means the effects of operation a are visible to operation b.

For example, if a wrote x = 5, and b later reads x = 5, then a is visible to b.

It’s acyclic, meaning visibility never loops back (an operation can’t see itself through a chain).

If two writes are not visible to each other, it means they happened independently or concurrently — they didn’t know about each other.

2. Arbitration (ar)

This is a total order of all operations in the system — even concurrent ones.

It represents how the system decides which operation “wins” when there’s a conflict.

Example: If two users write to the same object at the same time, ar determines which one is treated as coming first.

This total order can come from:

Timestamps (Lamport clocks),

Consensus protocols (like Paxos or Raft),

Central servers,

Or deterministic conflict rules (e.g., “last write wins”).

So, vis shows what each process sees, and ar decides what the system resolves as final order.


The “Happens-Before” Order (hb)
The paper defines:
hb=(so∪vis)+

This means:
hb (happens-before) combines two things:


so (session order): the order of operations by the same user/session.


vis (visibility): which operations’ effects are visible.


The + means “take the transitive closure,” i.e., chain them all together.


So hb captures causal ordering — which operations definitely happened before others in the overall system view.
Contexts and Return Values
To check if the system behaves “correctly,” we define a context for each operation.
 A context = the subset of the abstract execution that’s visible to that operation.
Formally:
cxt(A,op)=A∣op,vis−1(op), vis, ar

That means:
 for operation op, its context includes:
Itself,


All operations visible to it (vis^{-1}(op)),


The visibility relation (vis),


The arbitration order (ar).


So, the context is like the “local view” of the system that op can see when it runs.

Imagine 3 operations in a distributed system:
w1: write(x = 5)


w2: write(x = 10)


r1: read(x)


Now suppose:
w1 vis→ w2 (w2 saw w1’s effect)


w2 vis→ r1 (r1 saw w2’s effect)


But w1 is not directly visible to r1 (¬(w1 vis→ r1))


Then:
vis⁻¹(r1) = {w2}
 → only w2 is directly visible to r1.
A replicated data type (like a register, counter, or set) has a function F that defines what values operations should return depending on their context.
For example:
If you’re reading from a register, F says what value you should get based on the last visible write.


Return Value Consistency (RVAL) means:
Every operation returns a value that matches what the data type says it should return, given its context.
A consistency predicate P is just a logical condition (a rule) that must hold in A.
H⊨P1​∧…∧Pn​⇔∃A:H(A)=H∧A⊨P1​∧…∧Pn​
H satisfies P1 and … and Pn if and only if there exists an abstract execution A such that the history of A equals H, and A satisfies all P1 … Pn.

